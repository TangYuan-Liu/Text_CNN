{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, Input, concatenate\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from keras import optimizers\n",
    "from pathlib import Path\n",
    "import jieba\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GRU\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dict='C:/Users/my-pc/Desktop/problem1/newdict.txt'\n",
    "# with open(dict,'r') as dictfile:\n",
    "#     dicf=dictfile.readlines()\n",
    "#     for item in dicf:\n",
    "#       jieba.del_word(item.replace('\\n',''))\n",
    "#       print(item)\n",
    "\n",
    "path_train = \"C:\\\\Users\\\\my-pc\\\\Desktop\\\\problem3\"\n",
    " \n",
    "files_train = skds.load_files(path_train,load_content=False)\n",
    " \n",
    "label_index = files_train.target\n",
    "label_names = files_train.target_names\n",
    "labelled_files = files_train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"answers\"]\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],Path(f).read_text()))\n",
    "    i += 1\n",
    " \n",
    "# We have training data available as dictionary filename, category, data\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1478"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['answers'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,data['answers'].shape[0]):\n",
    "    data['answers'].iloc[i]=data['answers'].iloc[i].replace(' ','')\n",
    "    data['answers'].iloc[i]=data['answers'].iloc[i].replace(',','')\n",
    "    data['answers'].iloc[i]=data['answers'].iloc[i].replace('.','')\n",
    "    data['answers'].iloc[i]=data['answers'].iloc[i].replace('？','')\n",
    "    data['answers'].iloc[i]=data['answers'].iloc[i].replace('，','')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ds=[]\n",
    "# for item in data['answers']:\n",
    "#     text=str(item).lower()\n",
    "# #     print(text)\n",
    "#     chinese=\"\".join(i for i in text if ord(i) >= 256)\n",
    "#     chlist=[]\n",
    "#     for i in chinese:\n",
    "#         chlist.append(i)\n",
    "\n",
    "#     enlist=[]\n",
    "#     for i in text:\n",
    "#         if ord(i) >=256:\n",
    "#             text=text.replace(i,' ')\n",
    "#     enlist=list(nltk.word_tokenize(text))\n",
    "#     listall=enlist+chlist\n",
    "# #     print(listall)\n",
    "#     ds.append(listall)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stopfile=\"C:\\\\Users\\\\my-pc\\\\Desktop\\\\problem1\\\\stopwords.txt\"\n",
    "# with open(stopfile,'r',encoding='UTF-8') as f:\n",
    "#    fs=f.readlines()\n",
    "# for i in range(0,len(fs)):\n",
    "#     fs[i]=fs[i].replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(0,data.shape[0]):\n",
    "#     ds[i]=[i for i in ds[i] if i not in fs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = 6\n",
    "vocab_size = 2000\n",
    "batch_size = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size,char_level=True)\n",
    "tokenizer.fit_on_texts(data['answers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+': 592,\n",
       " '0': 476,\n",
       " '1': 305,\n",
       " '3': 717,\n",
       " '6': 678,\n",
       " '7': 414,\n",
       " '8': 401,\n",
       " 'a': 8,\n",
       " 'b': 376,\n",
       " 'c': 93,\n",
       " 'd': 146,\n",
       " 'e': 64,\n",
       " 'f': 261,\n",
       " 'g': 174,\n",
       " 'h': 173,\n",
       " 'i': 119,\n",
       " 'j': 15,\n",
       " 'k': 195,\n",
       " 'l': 199,\n",
       " 'm': 40,\n",
       " 'n': 102,\n",
       " 'o': 79,\n",
       " 'p': 120,\n",
       " 'q': 621,\n",
       " 'r': 97,\n",
       " 's': 90,\n",
       " 't': 84,\n",
       " 'u': 159,\n",
       " 'v': 14,\n",
       " 'w': 243,\n",
       " 'x': 407,\n",
       " 'y': 269,\n",
       " 'z': 607,\n",
       " '、': 18,\n",
       " '。': 9,\n",
       " '一': 12,\n",
       " '七': 471,\n",
       " '三': 77,\n",
       " '上': 178,\n",
       " '下': 133,\n",
       " '不': 68,\n",
       " '与': 257,\n",
       " '专': 481,\n",
       " '且': 381,\n",
       " '业': 609,\n",
       " '东': 177,\n",
       " '两': 116,\n",
       " '个': 4,\n",
       " '中': 51,\n",
       " '串': 370,\n",
       " '临': 334,\n",
       " '为': 21,\n",
       " '主': 32,\n",
       " '久': 127,\n",
       " '么': 82,\n",
       " '义': 225,\n",
       " '之': 123,\n",
       " '乎': 515,\n",
       " '九': 559,\n",
       " '也': 101,\n",
       " '习': 572,\n",
       " '书': 564,\n",
       " '了': 78,\n",
       " '二': 158,\n",
       " '于': 131,\n",
       " '云': 590,\n",
       " '互': 473,\n",
       " '五': 74,\n",
       " '些': 36,\n",
       " '亡': 588,\n",
       " '交': 409,\n",
       " '产': 341,\n",
       " '享': 106,\n",
       " '亲': 594,\n",
       " '人': 413,\n",
       " '什': 100,\n",
       " '仅': 612,\n",
       " '介': 703,\n",
       " '从': 223,\n",
       " '他': 259,\n",
       " '代': 39,\n",
       " '令': 163,\n",
       " '以': 65,\n",
       " '们': 109,\n",
       " '件': 321,\n",
       " '价': 746,\n",
       " '任': 598,\n",
       " '份': 549,\n",
       " '伊': 399,\n",
       " '优': 451,\n",
       " '会': 85,\n",
       " '似': 229,\n",
       " '但': 226,\n",
       " '位': 339,\n",
       " '低': 657,\n",
       " '住': 654,\n",
       " '体': 251,\n",
       " '何': 599,\n",
       " '余': 454,\n",
       " '作': 125,\n",
       " '你': 264,\n",
       " '使': 249,\n",
       " '例': 157,\n",
       " '供': 294,\n",
       " '依': 436,\n",
       " '便': 447,\n",
       " '保': 201,\n",
       " '信': 124,\n",
       " '修': 460,\n",
       " '候': 156,\n",
       " '值': 244,\n",
       " '偏': 634,\n",
       " '做': 248,\n",
       " '储': 81,\n",
       " '像': 115,\n",
       " '允': 740,\n",
       " '元': 342,\n",
       " '充': 695,\n",
       " '先': 213,\n",
       " '免': 699,\n",
       " '入': 238,\n",
       " '全': 246,\n",
       " '八': 327,\n",
       " '公': 393,\n",
       " '六': 353,\n",
       " '共': 89,\n",
       " '关': 250,\n",
       " '其': 108,\n",
       " '具': 281,\n",
       " '典': 682,\n",
       " '养': 562,\n",
       " '兼': 724,\n",
       " '内': 17,\n",
       " '册': 715,\n",
       " '再': 235,\n",
       " '写': 441,\n",
       " '冲': 425,\n",
       " '决': 652,\n",
       " '况': 449,\n",
       " '减': 668,\n",
       " '几': 83,\n",
       " '出': 141,\n",
       " '函': 273,\n",
       " '分': 16,\n",
       " '切': 461,\n",
       " '划': 241,\n",
       " '列': 408,\n",
       " '则': 431,\n",
       " '刚': 467,\n",
       " '创': 179,\n",
       " '初': 427,\n",
       " '判': 546,\n",
       " '利': 545,\n",
       " '别': 208,\n",
       " '到': 128,\n",
       " '制': 211,\n",
       " '前': 167,\n",
       " '剩': 513,\n",
       " '力': 435,\n",
       " '功': 385,\n",
       " '加': 138,\n",
       " '务': 227,\n",
       " '动': 189,\n",
       " '势': 556,\n",
       " '包': 103,\n",
       " '化': 307,\n",
       " '区': 1,\n",
       " '十': 539,\n",
       " '升': 677,\n",
       " '单': 303,\n",
       " '占': 418,\n",
       " '卫': 713,\n",
       " '印': 487,\n",
       " '即': 538,\n",
       " '卸': 721,\n",
       " '厂': 660,\n",
       " '压': 362,\n",
       " '原': 335,\n",
       " '去': 206,\n",
       " '参': 288,\n",
       " '又': 140,\n",
       " '叉': 610,\n",
       " '及': 110,\n",
       " '反': 448,\n",
       " '发': 263,\n",
       " '取': 312,\n",
       " '变': 63,\n",
       " '口': 267,\n",
       " '句': 512,\n",
       " '另': 256,\n",
       " '只': 197,\n",
       " '叫': 171,\n",
       " '可': 94,\n",
       " '台': 542,\n",
       " '号': 319,\n",
       " '各': 222,\n",
       " '合': 524,\n",
       " '同': 219,\n",
       " '名': 306,\n",
       " '后': 26,\n",
       " '向': 297,\n",
       " '吗': 290,\n",
       " '否': 604,\n",
       " '吧': 67,\n",
       " '含': 258,\n",
       " '启': 488,\n",
       " '呀': 186,\n",
       " '呃': 25,\n",
       " '告': 508,\n",
       " '呐': 390,\n",
       " '员': 380,\n",
       " '呢': 88,\n",
       " '周': 331,\n",
       " '呵': 442,\n",
       " '命': 328,\n",
       " '和': 37,\n",
       " '咨': 565,\n",
       " '咯': 388,\n",
       " '咳': 575,\n",
       " '哈': 577,\n",
       " '响': 602,\n",
       " '哎': 419,\n",
       " '哦': 268,\n",
       " '哪': 145,\n",
       " '唯': 311,\n",
       " '商': 661,\n",
       " '啊': 49,\n",
       " '啥': 495,\n",
       " '啦': 309,\n",
       " '喂': 534,\n",
       " '喽': 630,\n",
       " '嗯': 23,\n",
       " '嘛': 134,\n",
       " '嘞': 619,\n",
       " '器': 31,\n",
       " '四': 149,\n",
       " '回': 143,\n",
       " '因': 271,\n",
       " '团': 747,\n",
       " '园': 536,\n",
       " '固': 613,\n",
       " '图': 647,\n",
       " '在': 57,\n",
       " '地': 28,\n",
       " '场': 496,\n",
       " '圾': 169,\n",
       " '址': 193,\n",
       " '均': 674,\n",
       " '块': 111,\n",
       " '垃': 168,\n",
       " '型': 121,\n",
       " '城': 736,\n",
       " '域': 38,\n",
       " '基': 135,\n",
       " '堆': 10,\n",
       " '境': 468,\n",
       " '增': 608,\n",
       " '声': 493,\n",
       " '处': 324,\n",
       " '备': 573,\n",
       " '复': 322,\n",
       " '夕': 651,\n",
       " '外': 232,\n",
       " '多': 191,\n",
       " '够': 541,\n",
       " '大': 104,\n",
       " '天': 629,\n",
       " '太': 230,\n",
       " '头': 606,\n",
       " '好': 182,\n",
       " '如': 118,\n",
       " '始': 377,\n",
       " '子': 349,\n",
       " '字': 164,\n",
       " '存': 11,\n",
       " '学': 738,\n",
       " '它': 46,\n",
       " '安': 581,\n",
       " '完': 277,\n",
       " '定': 183,\n",
       " '实': 99,\n",
       " '室': 725,\n",
       " '家': 464,\n",
       " '容': 345,\n",
       " '寄': 234,\n",
       " '密': 623,\n",
       " '对': 48,\n",
       " '导': 656,\n",
       " '封': 653,\n",
       " '射': 670,\n",
       " '将': 308,\n",
       " '小': 299,\n",
       " '少': 355,\n",
       " '尬': 709,\n",
       " '就': 24,\n",
       " '尴': 708,\n",
       " '局': 130,\n",
       " '层': 337,\n",
       " '展': 432,\n",
       " '属': 296,\n",
       " '岗': 561,\n",
       " '工': 382,\n",
       " '差': 332,\n",
       " '己': 295,\n",
       " '已': 323,\n",
       " '市': 737,\n",
       " '布': 748,\n",
       " '帧': 176,\n",
       " '常': 52,\n",
       " '干': 731,\n",
       " '平': 348,\n",
       " '年': 73,\n",
       " '并': 366,\n",
       " '幸': 379,\n",
       " '幼': 666,\n",
       " '广': 635,\n",
       " '序': 35,\n",
       " '库': 375,\n",
       " '应': 132,\n",
       " '底': 469,\n",
       " '度': 363,\n",
       " '建': 175,\n",
       " '开': 279,\n",
       " '异': 212,\n",
       " '弃': 706,\n",
       " '式': 397,\n",
       " '引': 137,\n",
       " '强': 466,\n",
       " '当': 122,\n",
       " '录': 272,\n",
       " '形': 439,\n",
       " '影': 601,\n",
       " '往': 537,\n",
       " '很': 210,\n",
       " '得': 214,\n",
       " '循': 394,\n",
       " '微': 558,\n",
       " '心': 636,\n",
       " '必': 570,\n",
       " '忆': 544,\n",
       " '志': 640,\n",
       " '忘': 316,\n",
       " '快': 429,\n",
       " '念': 520,\n",
       " '忽': 722,\n",
       " '态': 56,\n",
       " '怎': 378,\n",
       " '思': 510,\n",
       " '性': 387,\n",
       " '总': 330,\n",
       " '恢': 486,\n",
       " '息': 126,\n",
       " '悉': 422,\n",
       " '情': 423,\n",
       " '惯': 637,\n",
       " '想': 325,\n",
       " '意': 491,\n",
       " '愿': 745,\n",
       " '懂': 701,\n",
       " '成': 165,\n",
       " '我': 87,\n",
       " '或': 192,\n",
       " '户': 527,\n",
       " '所': 105,\n",
       " '才': 529,\n",
       " '执': 76,\n",
       " '扩': 553,\n",
       " '扫': 658,\n",
       " '找': 551,\n",
       " '承': 514,\n",
       " '技': 300,\n",
       " '把': 266,\n",
       " '抛': 406,\n",
       " '护': 691,\n",
       " '报': 535,\n",
       " '抱': 729,\n",
       " '拉': 712,\n",
       " '拗': 595,\n",
       " '拟': 42,\n",
       " '拥': 522,\n",
       " '择': 530,\n",
       " '括': 113,\n",
       " '拿': 642,\n",
       " '持': 247,\n",
       " '挂': 720,\n",
       " '指': 112,\n",
       " '按': 583,\n",
       " '挥': 743,\n",
       " '挺': 696,\n",
       " '换': 352,\n",
       " '据': 71,\n",
       " '掉': 540,\n",
       " '接': 181,\n",
       " '控': 614,\n",
       " '描': 347,\n",
       " '提': 293,\n",
       " '搜': 664,\n",
       " '操': 152,\n",
       " '擎': 392,\n",
       " '支': 410,\n",
       " '收': 144,\n",
       " '改': 417,\n",
       " '放': 59,\n",
       " '效': 557,\n",
       " '数': 22,\n",
       " '整': 283,\n",
       " '文': 298,\n",
       " '料': 675,\n",
       " '断': 547,\n",
       " '新': 92,\n",
       " '方': 5,\n",
       " '无': 371,\n",
       " '既': 638,\n",
       " '日': 639,\n",
       " '旧': 462,\n",
       " '时': 69,\n",
       " '明': 386,\n",
       " '映': 503,\n",
       " '是': 6,\n",
       " '显': 711,\n",
       " '普': 683,\n",
       " '景': 673,\n",
       " '暂': 597,\n",
       " '更': 428,\n",
       " '替': 620,\n",
       " '最': 170,\n",
       " '有': 13,\n",
       " '服': 236,\n",
       " '朝': 650,\n",
       " '期': 265,\n",
       " '未': 574,\n",
       " '本': 29,\n",
       " '术': 301,\n",
       " '机': 45,\n",
       " '束': 367,\n",
       " '杠': 507,\n",
       " '条': 240,\n",
       " '来': 75,\n",
       " '构': 292,\n",
       " '析': 579,\n",
       " '果': 224,\n",
       " '某': 521,\n",
       " '查': 494,\n",
       " '标': 357,\n",
       " '栈': 3,\n",
       " '树': 452,\n",
       " '校': 688,\n",
       " '样': 150,\n",
       " '核': 528,\n",
       " '根': 320,\n",
       " '桟': 438,\n",
       " '桢': 446,\n",
       " '梳': 632,\n",
       " '检': 523,\n",
       " '楚': 209,\n",
       " '概': 198,\n",
       " '模': 237,\n",
       " '次': 356,\n",
       " '歉': 730,\n",
       " '止': 655,\n",
       " '正': 326,\n",
       " '此': 443,\n",
       " '步': 389,\n",
       " '死': 531,\n",
       " '段': 239,\n",
       " '毁': 396,\n",
       " '每': 129,\n",
       " '比': 114,\n",
       " '毕': 719,\n",
       " '气': 631,\n",
       " '永': 142,\n",
       " '求': 434,\n",
       " '汇': 569,\n",
       " '池': 117,\n",
       " '没': 202,\n",
       " '法': 7,\n",
       " '注': 456,\n",
       " '活': 346,\n",
       " '流': 474,\n",
       " '测': 517,\n",
       " '浮': 641,\n",
       " '消': 532,\n",
       " '涉': 501,\n",
       " '深': 490,\n",
       " '清': 185,\n",
       " '源': 289,\n",
       " '溜': 727,\n",
       " '溢': 364,\n",
       " '溯': 667,\n",
       " '满': 518,\n",
       " '滴': 560,\n",
       " '灵': 718,\n",
       " '点': 218,\n",
       " '然': 30,\n",
       " '照': 533,\n",
       " '熟': 391,\n",
       " '片': 580,\n",
       " '版': 457,\n",
       " '物': 480,\n",
       " '特': 424,\n",
       " '状': 479,\n",
       " '独': 315,\n",
       " '率': 511,\n",
       " '环': 318,\n",
       " '现': 207,\n",
       " '理': 160,\n",
       " '生': 70,\n",
       " '用': 53,\n",
       " '由': 270,\n",
       " '申': 445,\n",
       " '甸': 400,\n",
       " '界': 554,\n",
       " '留': 497,\n",
       " '略': 593,\n",
       " '疑': 693,\n",
       " '百': 665,\n",
       " '的': 2,\n",
       " '监': 596,\n",
       " '盘': 645,\n",
       " '目': 384,\n",
       " '直': 245,\n",
       " '相': 203,\n",
       " '看': 274,\n",
       " '真': 516,\n",
       " '着': 260,\n",
       " '知': 221,\n",
       " '短': 589,\n",
       " '码': 80,\n",
       " '研': 430,\n",
       " '础': 291,\n",
       " '硬': 644,\n",
       " '确': 710,\n",
       " '示': 317,\n",
       " '离': 374,\n",
       " '私': 147,\n",
       " '秉': 716,\n",
       " '种': 136,\n",
       " '称': 284,\n",
       " '移': 359,\n",
       " '程': 19,\n",
       " '稍': 646,\n",
       " '稳': 698,\n",
       " '究': 453,\n",
       " '空': 200,\n",
       " '立': 548,\n",
       " '站': 450,\n",
       " '端': 412,\n",
       " '符': 302,\n",
       " '第': 96,\n",
       " '笼': 735,\n",
       " '等': 66,\n",
       " '答': 576,\n",
       " '策': 686,\n",
       " '简': 420,\n",
       " '算': 180,\n",
       " '管': 262,\n",
       " '类': 55,\n",
       " '粗': 563,\n",
       " '糊': 662,\n",
       " '系': 351,\n",
       " '素': 426,\n",
       " '索': 498,\n",
       " '级': 433,\n",
       " '纬': 723,\n",
       " '线': 54,\n",
       " '组': 220,\n",
       " '细': 333,\n",
       " '终': 687,\n",
       " '绍': 704,\n",
       " '经': 287,\n",
       " '结': 255,\n",
       " '给': 373,\n",
       " '绝': 663,\n",
       " '统': 344,\n",
       " '继': 437,\n",
       " '续': 404,\n",
       " '维': 690,\n",
       " '缓': 280,\n",
       " '编': 204,\n",
       " '缩': 726,\n",
       " '网': 749,\n",
       " '置': 304,\n",
       " '群': 600,\n",
       " '老': 91,\n",
       " '考': 679,\n",
       " '者': 190,\n",
       " '而': 228,\n",
       " '聚': 734,\n",
       " '肯': 628,\n",
       " '背': 689,\n",
       " '能': 139,\n",
       " '自': 161,\n",
       " '至': 444,\n",
       " '致': 313,\n",
       " '般': 72,\n",
       " '色': 616,\n",
       " '节': 242,\n",
       " '若': 611,\n",
       " '英': 567,\n",
       " '范': 369,\n",
       " '获': 506,\n",
       " '虑': 680,\n",
       " '虚': 41,\n",
       " '虽': 568,\n",
       " '蛇': 692,\n",
       " '行': 50,\n",
       " '补': 694,\n",
       " '表': 217,\n",
       " '被': 172,\n",
       " '装': 500,\n",
       " '西': 184,\n",
       " '要': 27,\n",
       " '见': 343,\n",
       " '规': 276,\n",
       " '视': 648,\n",
       " '觉': 458,\n",
       " '角': 499,\n",
       " '解': 253,\n",
       " '触': 340,\n",
       " '言': 336,\n",
       " '譬': 744,\n",
       " '计': 33,\n",
       " '认': 489,\n",
       " '讨': 681,\n",
       " '让': 643,\n",
       " '记': 155,\n",
       " '讲': 455,\n",
       " '许': 741,\n",
       " '论': 505,\n",
       " '设': 372,\n",
       " '访': 421,\n",
       " '证': 605,\n",
       " '识': 582,\n",
       " '诉': 509,\n",
       " '词': 463,\n",
       " '译': 216,\n",
       " '话': 47,\n",
       " '询': 566,\n",
       " '该': 151,\n",
       " '语': 329,\n",
       " '误': 504,\n",
       " '说': 86,\n",
       " '诶': 411,\n",
       " '请': 402,\n",
       " '诸': 742,\n",
       " '读': 603,\n",
       " '课': 728,\n",
       " '调': 196,\n",
       " '谈': 587,\n",
       " '谢': 502,\n",
       " '象': 58,\n",
       " '貌': 659,\n",
       " '账': 625,\n",
       " '质': 586,\n",
       " '费': 700,\n",
       " '资': 416,\n",
       " '赖': 483,\n",
       " '走': 685,\n",
       " '起': 338,\n",
       " '超': 543,\n",
       " '越': 550,\n",
       " '跑': 684,\n",
       " '跟': 148,\n",
       " '跳': 383,\n",
       " '身': 314,\n",
       " '转': 368,\n",
       " '轮': 697,\n",
       " '轻': 194,\n",
       " '载': 166,\n",
       " '较': 215,\n",
       " '辑': 465,\n",
       " '输': 672,\n",
       " '辟': 472,\n",
       " '边': 231,\n",
       " '达': 633,\n",
       " '过': 153,\n",
       " '运': 107,\n",
       " '近': 440,\n",
       " '返': 484,\n",
       " '还': 20,\n",
       " '这': 34,\n",
       " '进': 154,\n",
       " '远': 478,\n",
       " '连': 485,\n",
       " '迭': 702,\n",
       " '述': 358,\n",
       " '追': 622,\n",
       " '退': 714,\n",
       " '选': 475,\n",
       " '逐': 739,\n",
       " '通': 188,\n",
       " '速': 618,\n",
       " '造': 591,\n",
       " '逻': 492,\n",
       " '遇': 578,\n",
       " '道': 233,\n",
       " '那': 43,\n",
       " '部': 95,\n",
       " '都': 98,\n",
       " '配': 205,\n",
       " '采': 470,\n",
       " '释': 310,\n",
       " '里': 60,\n",
       " '重': 365,\n",
       " '量': 44,\n",
       " '针': 282,\n",
       " '链': 361,\n",
       " '销': 395,\n",
       " '锁': 627,\n",
       " '错': 403,\n",
       " '键': 415,\n",
       " '长': 285,\n",
       " '门': 482,\n",
       " '问': 278,\n",
       " '间': 162,\n",
       " '阐': 669,\n",
       " '队': 459,\n",
       " '防': 584,\n",
       " '附': 733,\n",
       " '际': 405,\n",
       " '降': 585,\n",
       " '限': 555,\n",
       " '除': 275,\n",
       " '随': 360,\n",
       " '隐': 676,\n",
       " '隔': 350,\n",
       " '集': 354,\n",
       " '零': 526,\n",
       " '需': 252,\n",
       " '青': 398,\n",
       " '静': 61,\n",
       " '非': 187,\n",
       " '面': 62,\n",
       " '音': 707,\n",
       " '项': 552,\n",
       " '顺': 626,\n",
       " '须': 571,\n",
       " '顾': 617,\n",
       " '预': 671,\n",
       " '领': 525,\n",
       " '频': 649,\n",
       " '题': 286,\n",
       " '颜': 615,\n",
       " '额': 732,\n",
       " '饰': 519,\n",
       " '首': 254,\n",
       " '高': 477,\n",
       " '鬼': 624,\n",
       " '默': 705}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'java这分成两个吧一个栈一个堆是这个吗哎想具体想不太清楚想不太起来。堆的话因为有一个指的是嗯嗯固定分配值的然后就是像一些嗯基础变量所在的地方还有就是非运算对象栈用的地方反正堆和栈啊呀具体分不清楚哪个是哪个了想不起来了。'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['answers'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=data['answers'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15],\n",
       " [8],\n",
       " [14],\n",
       " [8],\n",
       " [34],\n",
       " [16],\n",
       " [165],\n",
       " [116],\n",
       " [4],\n",
       " [67],\n",
       " [12],\n",
       " [4],\n",
       " [3],\n",
       " [12],\n",
       " [4],\n",
       " [10],\n",
       " [6],\n",
       " [34],\n",
       " [4],\n",
       " [290],\n",
       " [419],\n",
       " [325],\n",
       " [281],\n",
       " [251],\n",
       " [325],\n",
       " [68],\n",
       " [230],\n",
       " [185],\n",
       " [209],\n",
       " [325],\n",
       " [68],\n",
       " [230],\n",
       " [338],\n",
       " [75],\n",
       " [9],\n",
       " [10],\n",
       " [2],\n",
       " [47],\n",
       " [271],\n",
       " [21],\n",
       " [13],\n",
       " [12],\n",
       " [4],\n",
       " [112],\n",
       " [2],\n",
       " [6],\n",
       " [23],\n",
       " [23],\n",
       " [613],\n",
       " [183],\n",
       " [16],\n",
       " [205],\n",
       " [244],\n",
       " [2],\n",
       " [30],\n",
       " [26],\n",
       " [24],\n",
       " [6],\n",
       " [115],\n",
       " [12],\n",
       " [36],\n",
       " [23],\n",
       " [135],\n",
       " [291],\n",
       " [63],\n",
       " [44],\n",
       " [105],\n",
       " [57],\n",
       " [2],\n",
       " [28],\n",
       " [5],\n",
       " [20],\n",
       " [13],\n",
       " [24],\n",
       " [6],\n",
       " [187],\n",
       " [107],\n",
       " [180],\n",
       " [48],\n",
       " [58],\n",
       " [3],\n",
       " [53],\n",
       " [2],\n",
       " [28],\n",
       " [5],\n",
       " [448],\n",
       " [326],\n",
       " [10],\n",
       " [37],\n",
       " [3],\n",
       " [49],\n",
       " [186],\n",
       " [281],\n",
       " [251],\n",
       " [16],\n",
       " [68],\n",
       " [185],\n",
       " [209],\n",
       " [145],\n",
       " [4],\n",
       " [6],\n",
       " [145],\n",
       " [4],\n",
       " [78],\n",
       " [325],\n",
       " [68],\n",
       " [338],\n",
       " [75],\n",
       " [78],\n",
       " [9]]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(data['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pad_sequences(train_sequences, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_labels = 6\n",
    "# vocab_size = 5000\n",
    "# batch_size = 100\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=vocab_size,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "# tokenizer.fit_on_texts(train_posts)\n",
    "\n",
    "# x_train = tokenizer.texts_to_matrix(train_posts, mode='count')\n",
    "# x_test = tokenizer.texts_to_matrix(test_posts, mode='count')\n",
    "# encoder = LabelBinarizer()\n",
    "# encoder.fit(train_tags)\n",
    "# y_train = encoder.transform(train_tags)\n",
    "# y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       3\n",
       "2       2\n",
       "3       3\n",
       "4       4\n",
       "5       2\n",
       "6       2\n",
       "7       2\n",
       "8       3\n",
       "9       4\n",
       "10      2\n",
       "11      4\n",
       "12      4\n",
       "13      4\n",
       "14      2\n",
       "15      2\n",
       "16      1\n",
       "17      3\n",
       "18      2\n",
       "19      2\n",
       "20      2\n",
       "21      4\n",
       "22      4\n",
       "23      2\n",
       "24      2\n",
       "25      2\n",
       "26      4\n",
       "27      4\n",
       "28      2\n",
       "29      1\n",
       "       ..\n",
       "1448    5\n",
       "1449    4\n",
       "1450    1\n",
       "1451    3\n",
       "1452    2\n",
       "1453    3\n",
       "1454    3\n",
       "1455    1\n",
       "1456    3\n",
       "1457    1\n",
       "1458    3\n",
       "1459    2\n",
       "1460    3\n",
       "1461    1\n",
       "1462    1\n",
       "1463    2\n",
       "1464    2\n",
       "1465    2\n",
       "1466    4\n",
       "1467    2\n",
       "1468    4\n",
       "1469    2\n",
       "1470    2\n",
       "1471    3\n",
       "1472    4\n",
       "1473    3\n",
       "1474    3\n",
       "1475    4\n",
       "1476    2\n",
       "1477    2\n",
       "Name: category, Length: 1478, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(datas, data['category'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189     1\n",
       "1176    3\n",
       "506     3\n",
       "1244    2\n",
       "840     2\n",
       "566     1\n",
       "177     2\n",
       "1177    3\n",
       "413     4\n",
       "584     2\n",
       "766     2\n",
       "561     2\n",
       "200     2\n",
       "265     4\n",
       "1255    3\n",
       "383     3\n",
       "1128    2\n",
       "256     1\n",
       "26      4\n",
       "617     4\n",
       "1085    2\n",
       "916     1\n",
       "147     4\n",
       "1092    1\n",
       "1186    3\n",
       "518     4\n",
       "109     2\n",
       "397     2\n",
       "1431    3\n",
       "960     3\n",
       "       ..\n",
       "512     2\n",
       "280     1\n",
       "658     3\n",
       "1212    5\n",
       "1453    3\n",
       "52      4\n",
       "1306    3\n",
       "75      1\n",
       "606     2\n",
       "991     2\n",
       "360     2\n",
       "854     1\n",
       "1075    2\n",
       "364     2\n",
       "753     3\n",
       "137     4\n",
       "1185    4\n",
       "1086    4\n",
       "513     3\n",
       "629     4\n",
       "1097    5\n",
       "22      4\n",
       "1151    2\n",
       "714     2\n",
       "1374    1\n",
       "898     4\n",
       "1322    2\n",
       "1325    2\n",
       "1283    1\n",
       "102     2\n",
       "Name: category, Length: 1182, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_sequences = tokenizer.texts_to_sequences(str(x_train.iloc[0]))\n",
    "# x_test = tokenizer.texts_to_matrix(x_test, mode='count')\n",
    "encoder = LabelBinarizer()\n",
    "print(encoder.fit(data['category']))\n",
    "\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.zeros(x_train.shape[0])\n",
    "# for i in range(0,x_train.shape[0]):\n",
    "#     a[i]=len(data['answers'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train=np.column_stack((x_train,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b=np.zeros(x_test.shape[0])\n",
    "# for i in range(0,x_test.shape[0]):\n",
    "#     b[i]=len(data['answers'].iloc[i+x_train.shape[0]])\n",
    "# x_test=np.column_stack((x_test,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1087 samples, validate on 121 samples\n",
      "Epoch 1/30\n",
      "1087/1087 [==============================] - 31s 29ms/step - loss: 1.3117 - acc: 0.4002 - val_loss: 1.1159 - val_acc: 0.5455\n",
      "Epoch 2/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.9804 - acc: 0.5823 - val_loss: 0.8651 - val_acc: 0.6529\n",
      "Epoch 3/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.8779 - acc: 0.6247 - val_loss: 0.7702 - val_acc: 0.6860\n",
      "Epoch 4/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.8377 - acc: 0.6477 - val_loss: 0.7511 - val_acc: 0.7025\n",
      "Epoch 5/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.7908 - acc: 0.6624 - val_loss: 0.7071 - val_acc: 0.6860\n",
      "Epoch 6/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.7426 - acc: 0.6983 - val_loss: 0.6928 - val_acc: 0.7273\n",
      "Epoch 7/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.6955 - acc: 0.7019 - val_loss: 0.6975 - val_acc: 0.7273\n",
      "Epoch 8/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.6517 - acc: 0.7277 - val_loss: 0.7941 - val_acc: 0.7025\n",
      "Epoch 9/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.6467 - acc: 0.7424 - val_loss: 0.7378 - val_acc: 0.6694\n",
      "Epoch 10/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.6150 - acc: 0.7507 - val_loss: 0.7509 - val_acc: 0.6612\n",
      "Epoch 11/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.5630 - acc: 0.7755 - val_loss: 0.7170 - val_acc: 0.7107\n",
      "Epoch 12/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.5454 - acc: 0.7755 - val_loss: 0.7596 - val_acc: 0.6942\n",
      "Epoch 13/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.5447 - acc: 0.7866 - val_loss: 0.7226 - val_acc: 0.7025\n",
      "Epoch 14/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.5101 - acc: 0.7939 - val_loss: 0.7063 - val_acc: 0.7521\n",
      "Epoch 15/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.4550 - acc: 0.8252 - val_loss: 0.7142 - val_acc: 0.7438\n",
      "Epoch 16/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.4674 - acc: 0.8132 - val_loss: 0.7107 - val_acc: 0.7438\n",
      "Epoch 17/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.4458 - acc: 0.8197 - val_loss: 0.7456 - val_acc: 0.7438\n",
      "Epoch 18/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.4481 - acc: 0.8353 - val_loss: 0.7384 - val_acc: 0.7355\n",
      "Epoch 19/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.4264 - acc: 0.8298 - val_loss: 0.7705 - val_acc: 0.7107\n",
      "Epoch 20/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.4297 - acc: 0.8114 - val_loss: 0.7747 - val_acc: 0.7438\n",
      "Epoch 21/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.4184 - acc: 0.8280 - val_loss: 0.7569 - val_acc: 0.7355\n",
      "Epoch 22/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.3908 - acc: 0.8491 - val_loss: 0.8181 - val_acc: 0.7355\n",
      "Epoch 23/30\n",
      "1087/1087 [==============================] - 4s 3ms/step - loss: 0.3837 - acc: 0.8464 - val_loss: 0.8374 - val_acc: 0.7190\n",
      "Epoch 24/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.3647 - acc: 0.8611 - val_loss: 0.7842 - val_acc: 0.7190\n",
      "Epoch 25/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.3573 - acc: 0.8675 - val_loss: 0.7914 - val_acc: 0.7025\n",
      "Epoch 26/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.3460 - acc: 0.8703 - val_loss: 0.7939 - val_acc: 0.7273\n",
      "Epoch 27/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.3348 - acc: 0.8795 - val_loss: 0.8801 - val_acc: 0.7107\n",
      "Epoch 28/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.3237 - acc: 0.8730 - val_loss: 0.9000 - val_acc: 0.6777\n",
      "Epoch 29/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.3211 - acc: 0.8730 - val_loss: 0.9111 - val_acc: 0.6942\n",
      "Epoch 30/30\n",
      "1087/1087 [==============================] - 4s 4ms/step - loss: 0.2929 - acc: 0.8850 - val_loss: 0.8370 - val_acc: 0.7355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d78100c390>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(256, input_shape=(vocab_size,)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(256))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(256))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_labels))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.summary()\n",
    "# sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    " \n",
    "# history = model.fit(x_train, y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=120,\n",
    "#                     verbose=1,\n",
    "#                     validation_split=0.1)\n",
    "\n",
    "#  DNN模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 50, input_length=200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(100, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_split=0.1, epochs=30)\n",
    "\n",
    "# rnn lstm分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cnn(maxlen=200, max_features=2000, embed_size=32):\n",
    "    # Inputs\n",
    "    seq = Input(shape=[maxlen], name='x_seq')\n",
    "\n",
    "    # Embeddings layers\n",
    "    emb_comment = Embedding(max_features, embed_size)(seq)\n",
    "\n",
    "    # conv layers\n",
    "    convs = []\n",
    "    filter_sizes = [2,3, 4, 5]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=fsz, activation='relu')(emb_comment)\n",
    "\n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        \n",
    "#         inner = Reshape(target_shape=(( 100,1)), name='reshape')(l_pool)\n",
    "#         gru=GRU(64,dropout=0.5,recurrent_dropout=0.5,return_sequences=True)(inner)\n",
    "\n",
    "        convs.append(l_pool)\n",
    "    merge = concatenate(convs, axis=1)\n",
    "    #merge = Reshape(target_shape=((512)), name='reshape')(merge)\n",
    "    \n",
    "    out = Dropout(0.5)(merge)\n",
    "    output = Dense(32, activation='relu')(out)\n",
    "\n",
    "    output = Dense(units=6, activation='softmax')(output)\n",
    "\n",
    "    model = Model([seq], output)\n",
    "    #     adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 6)\n",
      "Train on 1063 samples, validate on 119 samples\n",
      "Epoch 1/50\n",
      "1063/1063 [==============================] - 3s 3ms/step - loss: 0.4329 - acc: 0.8333 - val_loss: 0.4137 - val_acc: 0.8333\n",
      "Epoch 2/50\n",
      "1063/1063 [==============================] - 0s 420us/step - loss: 0.3972 - acc: 0.8333 - val_loss: 0.3877 - val_acc: 0.8333\n",
      "Epoch 3/50\n",
      "1063/1063 [==============================] - 0s 415us/step - loss: 0.3719 - acc: 0.8333 - val_loss: 0.3495 - val_acc: 0.8333\n",
      "Epoch 4/50\n",
      "1063/1063 [==============================] - 0s 433us/step - loss: 0.3435 - acc: 0.8362 - val_loss: 0.3089 - val_acc: 0.8557\n",
      "Epoch 5/50\n",
      "1063/1063 [==============================] - 0s 435us/step - loss: 0.3111 - acc: 0.8628 - val_loss: 0.2700 - val_acc: 0.8964\n",
      "Epoch 6/50\n",
      "1063/1063 [==============================] - 0s 429us/step - loss: 0.2791 - acc: 0.8719 - val_loss: 0.2429 - val_acc: 0.9034\n",
      "Epoch 7/50\n",
      "1063/1063 [==============================] - 0s 421us/step - loss: 0.2564 - acc: 0.8866 - val_loss: 0.2207 - val_acc: 0.9132\n",
      "Epoch 8/50\n",
      "1063/1063 [==============================] - 0s 428us/step - loss: 0.2370 - acc: 0.8982 - val_loss: 0.2036 - val_acc: 0.9216\n",
      "Epoch 9/50\n",
      "1063/1063 [==============================] - 0s 427us/step - loss: 0.2227 - acc: 0.9051 - val_loss: 0.2006 - val_acc: 0.9188\n",
      "Epoch 10/50\n",
      "1063/1063 [==============================] - 0s 428us/step - loss: 0.2066 - acc: 0.9094 - val_loss: 0.1905 - val_acc: 0.9216\n",
      "Epoch 11/50\n",
      "1063/1063 [==============================] - 0s 421us/step - loss: 0.1967 - acc: 0.9144 - val_loss: 0.1846 - val_acc: 0.9188\n",
      "Epoch 12/50\n",
      "1063/1063 [==============================] - 0s 426us/step - loss: 0.1879 - acc: 0.9147 - val_loss: 0.1880 - val_acc: 0.9062\n",
      "Epoch 13/50\n",
      "1063/1063 [==============================] - 0s 422us/step - loss: 0.1883 - acc: 0.9133 - val_loss: 0.1795 - val_acc: 0.9230\n",
      "Epoch 14/50\n",
      "1063/1063 [==============================] - 0s 426us/step - loss: 0.1873 - acc: 0.9166 - val_loss: 0.1819 - val_acc: 0.9160\n",
      "Epoch 15/50\n",
      "1063/1063 [==============================] - 0s 421us/step - loss: 0.1799 - acc: 0.9158 - val_loss: 0.1801 - val_acc: 0.9216\n",
      "Epoch 16/50\n",
      "1063/1063 [==============================] - 0s 426us/step - loss: 0.1749 - acc: 0.9178 - val_loss: 0.1834 - val_acc: 0.9132\n",
      "Epoch 17/50\n",
      "1063/1063 [==============================] - 0s 428us/step - loss: 0.1693 - acc: 0.9205 - val_loss: 0.1828 - val_acc: 0.9188\n",
      "Epoch 18/50\n",
      "1063/1063 [==============================] - 0s 417us/step - loss: 0.1720 - acc: 0.9221 - val_loss: 0.1792 - val_acc: 0.9216\n",
      "Epoch 19/50\n",
      "1063/1063 [==============================] - 0s 448us/step - loss: 0.1667 - acc: 0.9233 - val_loss: 0.1896 - val_acc: 0.9188\n",
      "Epoch 20/50\n",
      "1063/1063 [==============================] - 0s 430us/step - loss: 0.1619 - acc: 0.9305 - val_loss: 0.1818 - val_acc: 0.9300\n",
      "Epoch 21/50\n",
      "1063/1063 [==============================] - 0s 437us/step - loss: 0.1607 - acc: 0.9271 - val_loss: 0.1902 - val_acc: 0.9132\n",
      "Epoch 22/50\n",
      "1063/1063 [==============================] - 0s 419us/step - loss: 0.1523 - acc: 0.9326 - val_loss: 0.1828 - val_acc: 0.9272\n",
      "Epoch 23/50\n",
      "1063/1063 [==============================] - 0s 431us/step - loss: 0.1498 - acc: 0.9327 - val_loss: 0.1858 - val_acc: 0.9174\n",
      "Epoch 24/50\n",
      "1063/1063 [==============================] - 0s 425us/step - loss: 0.1499 - acc: 0.9351 - val_loss: 0.1933 - val_acc: 0.9132\n",
      "Epoch 25/50\n",
      "1063/1063 [==============================] - 0s 420us/step - loss: 0.1479 - acc: 0.9378 - val_loss: 0.1846 - val_acc: 0.9258\n",
      "Epoch 26/50\n",
      "1063/1063 [==============================] - 0s 442us/step - loss: 0.1428 - acc: 0.9363 - val_loss: 0.1975 - val_acc: 0.9090\n",
      "Epoch 27/50\n",
      "1063/1063 [==============================] - 0s 429us/step - loss: 0.1513 - acc: 0.9340 - val_loss: 0.1889 - val_acc: 0.9202\n",
      "Epoch 28/50\n",
      "1063/1063 [==============================] - 0s 426us/step - loss: 0.1374 - acc: 0.9384 - val_loss: 0.1965 - val_acc: 0.9146\n",
      "Epoch 29/50\n",
      "1063/1063 [==============================] - 0s 416us/step - loss: 0.1367 - acc: 0.9374 - val_loss: 0.1921 - val_acc: 0.9118\n",
      "Epoch 30/50\n",
      "1063/1063 [==============================] - 0s 424us/step - loss: 0.1342 - acc: 0.9456 - val_loss: 0.1953 - val_acc: 0.9216\n",
      "Epoch 31/50\n",
      "1063/1063 [==============================] - 0s 424us/step - loss: 0.1322 - acc: 0.9429 - val_loss: 0.1938 - val_acc: 0.9132\n",
      "Epoch 32/50\n",
      "1063/1063 [==============================] - 0s 423us/step - loss: 0.1229 - acc: 0.9486 - val_loss: 0.1987 - val_acc: 0.9160\n",
      "Epoch 33/50\n",
      "1063/1063 [==============================] - 0s 430us/step - loss: 0.1266 - acc: 0.9448 - val_loss: 0.2074 - val_acc: 0.9104\n",
      "Epoch 34/50\n",
      "1063/1063 [==============================] - 0s 429us/step - loss: 0.1232 - acc: 0.9467 - val_loss: 0.2039 - val_acc: 0.9076\n",
      "Epoch 35/50\n",
      "1063/1063 [==============================] - 0s 448us/step - loss: 0.1209 - acc: 0.9481 - val_loss: 0.2103 - val_acc: 0.9048\n",
      "Epoch 36/50\n",
      "1063/1063 [==============================] - 0s 434us/step - loss: 0.1187 - acc: 0.9526 - val_loss: 0.2032 - val_acc: 0.9174\n",
      "Epoch 37/50\n",
      "1063/1063 [==============================] - 0s 430us/step - loss: 0.1158 - acc: 0.9506 - val_loss: 0.2123 - val_acc: 0.9076\n",
      "Epoch 38/50\n",
      "1063/1063 [==============================] - 0s 449us/step - loss: 0.1139 - acc: 0.9552 - val_loss: 0.2085 - val_acc: 0.9118\n",
      "Epoch 39/50\n",
      "1063/1063 [==============================] - 0s 432us/step - loss: 0.1127 - acc: 0.9530 - val_loss: 0.2095 - val_acc: 0.9076\n",
      "Epoch 40/50\n",
      "1063/1063 [==============================] - 0s 439us/step - loss: 0.1094 - acc: 0.9544 - val_loss: 0.2138 - val_acc: 0.9174\n",
      "Epoch 41/50\n",
      "1063/1063 [==============================] - 0s 444us/step - loss: 0.1050 - acc: 0.9570 - val_loss: 0.2221 - val_acc: 0.9062\n",
      "Epoch 42/50\n",
      "1063/1063 [==============================] - 0s 422us/step - loss: 0.1051 - acc: 0.9572 - val_loss: 0.2201 - val_acc: 0.9118\n",
      "Epoch 43/50\n",
      "1063/1063 [==============================] - 0s 431us/step - loss: 0.1025 - acc: 0.9553 - val_loss: 0.2209 - val_acc: 0.9062\n",
      "Epoch 44/50\n",
      "1063/1063 [==============================] - 0s 432us/step - loss: 0.1000 - acc: 0.9578 - val_loss: 0.2283 - val_acc: 0.9062\n",
      "Epoch 45/50\n",
      "1063/1063 [==============================] - 0s 444us/step - loss: 0.0997 - acc: 0.9616 - val_loss: 0.2280 - val_acc: 0.8992\n",
      "Epoch 46/50\n",
      "1063/1063 [==============================] - 0s 433us/step - loss: 0.1035 - acc: 0.9531 - val_loss: 0.2223 - val_acc: 0.9146\n",
      "Epoch 47/50\n",
      "1063/1063 [==============================] - 0s 436us/step - loss: 0.0938 - acc: 0.9661 - val_loss: 0.2333 - val_acc: 0.9034\n",
      "Epoch 48/50\n",
      "1063/1063 [==============================] - 0s 420us/step - loss: 0.0989 - acc: 0.9599 - val_loss: 0.2250 - val_acc: 0.9118\n",
      "Epoch 49/50\n",
      "1063/1063 [==============================] - 0s 430us/step - loss: 0.0911 - acc: 0.9647 - val_loss: 0.2335 - val_acc: 0.9076\n",
      "Epoch 50/50\n",
      "1063/1063 [==============================] - 0s 448us/step - loss: 0.0911 - acc: 0.9644 - val_loss: 0.2285 - val_acc: 0.9104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d8d12567f0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Reshape\n",
    "model = text_cnn()\n",
    "print(model.output_shape)\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "model.fit(x_train, y_train,\n",
    "          validation_split=0.1,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/296 [==============================] - 1s 2ms/step\n",
      "Test accuracy: 0.8941441629384015\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9886900e-01 1.0891416e-03 2.9914983e-05 1.1105255e-05 7.6881201e-07\n",
      "  1.1812241e-07]\n",
      " [2.7965560e-05 2.6663342e-01 7.3059374e-01 2.7014881e-03 4.3167725e-05\n",
      "  1.6569567e-07]\n",
      " [3.5901085e-04 9.7019508e-02 8.3780497e-01 6.3844338e-02 9.7183086e-04\n",
      "  3.6687405e-07]\n",
      " ...\n",
      " [4.0993422e-05 3.4091154e-01 6.5631878e-01 2.6072518e-03 1.2114404e-04\n",
      "  2.4991473e-07]\n",
      " [9.6909863e-01 2.9991871e-02 7.8648946e-04 1.0325833e-04 1.9300442e-05\n",
      "  4.3760619e-07]\n",
      " [9.8721328e-05 9.5956695e-01 4.0270951e-02 5.8449579e-05 4.7766366e-06\n",
      "  1.1101998e-07]]\n",
      "['1' '3' '3' ... '2' '1' '2']\n",
      "[[  0   0   0   0   0   0]\n",
      " [167   7   0   0   0   0]\n",
      " [  2 397  11   2   0   0]\n",
      " [  0  21 207  53   0   0]\n",
      " [  0   0  10 266   0   0]\n",
      " [  0   0   0   8  31   0]]\n"
     ]
    }
   ],
   "source": [
    "predict_test = model.predict(x_train)\n",
    "print(predict_test)\n",
    "predict = np.argmax(predict_test,axis=1)\n",
    "#axis = 1是取行的最大值的索引，0是列的最大值的索引\n",
    "y_train = encoder.inverse_transform(y_train)\n",
    "print(y_train)\n",
    "prenumbers=list(map(int, predict))\n",
    "uanumbers=list(map(int, y_train))\n",
    "print(confusion_matrix(uanumbers,prenumbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {}\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.01\n",
    "param['max_depth'] = 10\n",
    "\n",
    "param['n_estimators']=5000\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train=encoder.inverse_transform(y_train)\n",
    "y_test=encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(x_train, label=y_train)\n",
    "xg_test = xgb.DMatrix( x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.139073\ttest-merror:0.435644\n",
      "[1]\ttrain-merror:0.123344\ttest-merror:0.432343\n",
      "[2]\ttrain-merror:0.111755\ttest-merror:0.429043\n",
      "[3]\ttrain-merror:0.106788\ttest-merror:0.425743\n",
      "[4]\ttrain-merror:0.114238\ttest-merror:0.422442\n",
      "[5]\ttrain-merror:0.110099\ttest-merror:0.409241\n",
      "[6]\ttrain-merror:0.110927\ttest-merror:0.415842\n",
      "[7]\ttrain-merror:0.104305\ttest-merror:0.409241\n",
      "[8]\ttrain-merror:0.103477\ttest-merror:0.405941\n",
      "[9]\ttrain-merror:0.100993\ttest-merror:0.39604\n",
      "[10]\ttrain-merror:0.100993\ttest-merror:0.40264\n",
      "[11]\ttrain-merror:0.095199\ttest-merror:0.392739\n",
      "[12]\ttrain-merror:0.095199\ttest-merror:0.392739\n",
      "[13]\ttrain-merror:0.096854\ttest-merror:0.389439\n",
      "[14]\ttrain-merror:0.096854\ttest-merror:0.39604\n",
      "[15]\ttrain-merror:0.096854\ttest-merror:0.392739\n",
      "[16]\ttrain-merror:0.095199\ttest-merror:0.379538\n",
      "[17]\ttrain-merror:0.091887\ttest-merror:0.382838\n",
      "[18]\ttrain-merror:0.090232\ttest-merror:0.382838\n",
      "[19]\ttrain-merror:0.088576\ttest-merror:0.389439\n",
      "[20]\ttrain-merror:0.088576\ttest-merror:0.389439\n",
      "[21]\ttrain-merror:0.082781\ttest-merror:0.392739\n",
      "[22]\ttrain-merror:0.084437\ttest-merror:0.389439\n",
      "[23]\ttrain-merror:0.083609\ttest-merror:0.389439\n",
      "[24]\ttrain-merror:0.078642\ttest-merror:0.382838\n",
      "[25]\ttrain-merror:0.077815\ttest-merror:0.389439\n",
      "[26]\ttrain-merror:0.076987\ttest-merror:0.392739\n",
      "[27]\ttrain-merror:0.075331\ttest-merror:0.389439\n",
      "[28]\ttrain-merror:0.07202\ttest-merror:0.386139\n",
      "[29]\ttrain-merror:0.071192\ttest-merror:0.379538\n",
      "[30]\ttrain-merror:0.07202\ttest-merror:0.382838\n",
      "[31]\ttrain-merror:0.068709\ttest-merror:0.376238\n",
      "[32]\ttrain-merror:0.070364\ttest-merror:0.379538\n",
      "[33]\ttrain-merror:0.069536\ttest-merror:0.386139\n",
      "[34]\ttrain-merror:0.069536\ttest-merror:0.386139\n",
      "[35]\ttrain-merror:0.067881\ttest-merror:0.386139\n",
      "[36]\ttrain-merror:0.067053\ttest-merror:0.389439\n",
      "[37]\ttrain-merror:0.06457\ttest-merror:0.389439\n",
      "[38]\ttrain-merror:0.066225\ttest-merror:0.392739\n",
      "[39]\ttrain-merror:0.066225\ttest-merror:0.392739\n",
      "[40]\ttrain-merror:0.065397\ttest-merror:0.392739\n",
      "[41]\ttrain-merror:0.062914\ttest-merror:0.389439\n",
      "[42]\ttrain-merror:0.062914\ttest-merror:0.389439\n",
      "[43]\ttrain-merror:0.062086\ttest-merror:0.386139\n",
      "[44]\ttrain-merror:0.062914\ttest-merror:0.389439\n",
      "[45]\ttrain-merror:0.063742\ttest-merror:0.39604\n",
      "[46]\ttrain-merror:0.063742\ttest-merror:0.392739\n",
      "[47]\ttrain-merror:0.062086\ttest-merror:0.392739\n",
      "[48]\ttrain-merror:0.057947\ttest-merror:0.386139\n",
      "[49]\ttrain-merror:0.055464\ttest-merror:0.386139\n",
      "[50]\ttrain-merror:0.051325\ttest-merror:0.382838\n",
      "[51]\ttrain-merror:0.051325\ttest-merror:0.382838\n",
      "[52]\ttrain-merror:0.049669\ttest-merror:0.386139\n",
      "[53]\ttrain-merror:0.049669\ttest-merror:0.382838\n",
      "[54]\ttrain-merror:0.049669\ttest-merror:0.379538\n",
      "[55]\ttrain-merror:0.049669\ttest-merror:0.379538\n",
      "[56]\ttrain-merror:0.048841\ttest-merror:0.379538\n",
      "[57]\ttrain-merror:0.047185\ttest-merror:0.379538\n",
      "[58]\ttrain-merror:0.046358\ttest-merror:0.379538\n",
      "[59]\ttrain-merror:0.047185\ttest-merror:0.379538\n",
      "[60]\ttrain-merror:0.047185\ttest-merror:0.379538\n",
      "[61]\ttrain-merror:0.046358\ttest-merror:0.379538\n",
      "[62]\ttrain-merror:0.046358\ttest-merror:0.379538\n",
      "[63]\ttrain-merror:0.04553\ttest-merror:0.379538\n",
      "[64]\ttrain-merror:0.04553\ttest-merror:0.382838\n",
      "[65]\ttrain-merror:0.043874\ttest-merror:0.386139\n",
      "[66]\ttrain-merror:0.043874\ttest-merror:0.382838\n",
      "[67]\ttrain-merror:0.043046\ttest-merror:0.386139\n",
      "[68]\ttrain-merror:0.042219\ttest-merror:0.386139\n",
      "[69]\ttrain-merror:0.043046\ttest-merror:0.386139\n",
      "[70]\ttrain-merror:0.043046\ttest-merror:0.382838\n",
      "[71]\ttrain-merror:0.041391\ttest-merror:0.382838\n",
      "[72]\ttrain-merror:0.042219\ttest-merror:0.382838\n",
      "[73]\ttrain-merror:0.039735\ttest-merror:0.379538\n",
      "[74]\ttrain-merror:0.038907\ttest-merror:0.379538\n",
      "[75]\ttrain-merror:0.038079\ttest-merror:0.376238\n",
      "[76]\ttrain-merror:0.038079\ttest-merror:0.376238\n",
      "[77]\ttrain-merror:0.038079\ttest-merror:0.376238\n",
      "[78]\ttrain-merror:0.037252\ttest-merror:0.379538\n",
      "[79]\ttrain-merror:0.036424\ttest-merror:0.379538\n",
      "[80]\ttrain-merror:0.034768\ttest-merror:0.379538\n",
      "[81]\ttrain-merror:0.033113\ttest-merror:0.376238\n",
      "[82]\ttrain-merror:0.032285\ttest-merror:0.379538\n",
      "[83]\ttrain-merror:0.032285\ttest-merror:0.379538\n",
      "[84]\ttrain-merror:0.032285\ttest-merror:0.376238\n",
      "[85]\ttrain-merror:0.031457\ttest-merror:0.376238\n",
      "[86]\ttrain-merror:0.031457\ttest-merror:0.376238\n",
      "[87]\ttrain-merror:0.031457\ttest-merror:0.376238\n",
      "[88]\ttrain-merror:0.031457\ttest-merror:0.372937\n",
      "[89]\ttrain-merror:0.031457\ttest-merror:0.372937\n",
      "[90]\ttrain-merror:0.031457\ttest-merror:0.372937\n",
      "[91]\ttrain-merror:0.031457\ttest-merror:0.372937\n",
      "[92]\ttrain-merror:0.031457\ttest-merror:0.369637\n",
      "[93]\ttrain-merror:0.031457\ttest-merror:0.366337\n",
      "[94]\ttrain-merror:0.031457\ttest-merror:0.366337\n",
      "[95]\ttrain-merror:0.031457\ttest-merror:0.366337\n",
      "[96]\ttrain-merror:0.031457\ttest-merror:0.366337\n",
      "[97]\ttrain-merror:0.029801\ttest-merror:0.366337\n",
      "[98]\ttrain-merror:0.029801\ttest-merror:0.366337\n",
      "[99]\ttrain-merror:0.028974\ttest-merror:0.366337\n",
      "[100]\ttrain-merror:0.028974\ttest-merror:0.366337\n",
      "[101]\ttrain-merror:0.028146\ttest-merror:0.363036\n",
      "[102]\ttrain-merror:0.027318\ttest-merror:0.359736\n",
      "[103]\ttrain-merror:0.027318\ttest-merror:0.359736\n",
      "[104]\ttrain-merror:0.02649\ttest-merror:0.356436\n",
      "[105]\ttrain-merror:0.02649\ttest-merror:0.356436\n",
      "[106]\ttrain-merror:0.02649\ttest-merror:0.356436\n",
      "[107]\ttrain-merror:0.02649\ttest-merror:0.356436\n",
      "[108]\ttrain-merror:0.02649\ttest-merror:0.359736\n",
      "[109]\ttrain-merror:0.02649\ttest-merror:0.356436\n",
      "[110]\ttrain-merror:0.025662\ttest-merror:0.356436\n",
      "[111]\ttrain-merror:0.024834\ttest-merror:0.356436\n",
      "[112]\ttrain-merror:0.024834\ttest-merror:0.359736\n",
      "[113]\ttrain-merror:0.024834\ttest-merror:0.363036\n",
      "[114]\ttrain-merror:0.024834\ttest-merror:0.359736\n",
      "[115]\ttrain-merror:0.024834\ttest-merror:0.359736\n",
      "[116]\ttrain-merror:0.024007\ttest-merror:0.359736\n",
      "[117]\ttrain-merror:0.024007\ttest-merror:0.363036\n",
      "[118]\ttrain-merror:0.023179\ttest-merror:0.363036\n",
      "[119]\ttrain-merror:0.023179\ttest-merror:0.356436\n"
     ]
    }
   ],
   "source": [
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 120\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sklearn.metrics.confusion_matrix(\n",
    "#     y_test,   # array, Gound true (correct) target values\n",
    "#     test_pred,  # array, Estimated targets as returned by a classifier\n",
    "#     labels=None,  # array, List of labels to index the matrix.\n",
    "#     sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 3. 4. 1. 4. 3. 3. 2. 3. 4. 4. 3. 4. 3. 4. 4. 4. 2. 4. 3. 4. 3. 3. 4.\n",
      " 4. 4. 3. 1. 4. 3. 2. 3. 1. 2. 3. 1. 4. 2. 5. 3. 2. 3. 4. 3. 2. 3. 4. 4.\n",
      " 3. 3. 3. 3. 3. 4. 3. 3. 4. 4. 3. 1. 3. 3. 4. 3. 4. 4. 4. 4. 3. 4. 3. 4.\n",
      " 4. 3. 4. 4. 4. 4. 3. 3. 3. 3. 3. 3. 4. 3. 2. 3. 3. 3. 3. 3. 3. 3. 4. 5.\n",
      " 4. 4. 4. 3. 4. 4. 3. 2. 4. 2. 4. 5. 3. 3. 3. 3. 4. 3. 3. 4. 2. 3. 5. 3.\n",
      " 3. 4. 4. 3. 1. 3. 3. 2. 3. 3. 4. 3. 3. 1. 3. 4. 3. 4. 3. 3. 4. 3. 4. 4.\n",
      " 4. 3. 1. 4. 3. 3. 3. 3. 2. 4. 3. 3. 4. 2. 4. 4. 4. 2. 3. 3. 3. 3. 1. 4.\n",
      " 4. 3. 4. 3. 4. 4. 4. 1. 4. 3. 4. 3. 2. 4. 4. 4. 4. 4. 4. 4. 4. 1. 2. 3.\n",
      " 4. 3. 3. 3. 3. 3. 4. 3. 3. 1. 2. 2. 4. 3. 5. 3. 3. 4. 4. 3. 4. 4. 3. 4.\n",
      " 2. 3. 4. 4. 4. 4. 3. 4. 3. 3. 2. 4. 3. 4. 3. 1. 2. 3. 4. 1. 2. 5. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 4. 3. 4. 3. 3. 3. 3. 4. 4. 4. 3. 4. 3.\n",
      " 4. 3. 3. 4. 3. 2. 3. 3. 4. 3. 2. 3. 2. 2. 1. 3. 4. 3. 3. 4. 4. 4. 2. 3.\n",
      " 3. 1. 1. 4. 3. 3. 4. 4. 3. 3. 4. 3. 4. 4. 3.]\n",
      "4.0\n",
      "[[14  3  0  0  0]\n",
      " [ 4 20  4  0  0]\n",
      " [ 1 22 87 29  2]\n",
      " [ 0  3 27 72  9]\n",
      " [ 0  1  0  3  2]]\n"
     ]
    }
   ],
   "source": [
    "ctrain=xgb.DMatrix(x_test)\n",
    "predict_test = bst.predict(ctrain)\n",
    "print(predict_test)\n",
    "uanumbers=list(map(int, predict_test))\n",
    "umbers = list(map(int, y_test))\n",
    "print(predict_test[4])\n",
    "print(confusion_matrix(uanumbers,umbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
